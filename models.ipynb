{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch import nn\n",
    "\n",
    "from data import get_loaders, load_dataset\n",
    "from training import test_step, train_step, train_model, get_test_predictions\n",
    "from utils import print_losses\n",
    "from swag import sample_from_SWAG, run_SWAG\n",
    "from definitions import DATASETS\n",
    "from models import create_model, load_model\n",
    "from metrics import RMSE\n",
    "from swag_mod import calculate_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_mod(\n",
    "    batch_x: torch.Tensor,\n",
    "    batch_y: torch.Tensor,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion,\n",
    "    lr_multipliers,\n",
    "):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(batch_x)\n",
    "    loss = criterion(outputs, batch_y)\n",
    "    loss.backward()\n",
    "    multitiply_grads(model, lr_multipliers)\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train_SWAG_mod(\n",
    "    x_train: np.array,\n",
    "    y_train: np.array,\n",
    "    x_test: np.array,\n",
    "    y_test: np.array,\n",
    "    model: nn.Module,\n",
    "    K,\n",
    "    lr_multipliers,\n",
    "    epochs: int = 100,\n",
    "    batch_size: int = 100,\n",
    "    lr: float = 0.1,\n",
    "    verbose: bool = True,\n",
    "    c=1,\n",
    "    momentum=0,\n",
    "    weight_decay=0,\n",
    "):\n",
    "    assert c >= 1 and K >= 2\n",
    "    train_loader, test_loader = get_loaders(x_train, y_train, x_test, y_test, batch_size)\n",
    "    theta_epoch = torch.nn.utils.parameters_to_vector(model.parameters()).detach().cpu().clone()\n",
    "    theta = theta_epoch.clone()\n",
    "    theta_square = theta_epoch.clone() ** 2\n",
    "    D = None\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_losses = [test_step(batch_x, batch_y, model, criterion) for batch_x, batch_y in train_loader]\n",
    "    test_losses = [test_step(batch_x, batch_y, model, criterion) for batch_x, batch_y in test_loader]\n",
    "    if verbose:\n",
    "        print_losses(0, train_losses, test_losses)\n",
    "\n",
    "    thetas = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_losses = [train_step_mod(batch_x, batch_y, model, optimizer, criterion, lr_multipliers) \n",
    "                        for batch_x, batch_y in train_loader]\n",
    "        test_losses = [test_step(batch_x, batch_y, model, criterion) for batch_x, batch_y in test_loader]\n",
    "        if verbose:\n",
    "            print_losses(epoch, train_losses, test_losses)\n",
    "        if epoch % c == 0:\n",
    "            if verbose:\n",
    "                print(\"SWAG moment update\")\n",
    "            n = epoch / c\n",
    "            theta_epoch = torch.nn.utils.parameters_to_vector(model.parameters()).detach().cpu().clone()\n",
    "            thetas.append(theta_epoch.clone())\n",
    "            theta = (n * theta + theta_epoch.clone()) / (n + 1)\n",
    "            theta_square = (n * theta_square + theta_epoch.clone() ** 2) / (n + 1)\n",
    "            deviations = (theta_epoch.clone() - theta).reshape(-1, 1)\n",
    "            if D is None:\n",
    "                D = deviations\n",
    "            else:\n",
    "                if D.shape[1] == K:\n",
    "                    D = D[:, 1:]\n",
    "                D = torch.cat((D, deviations), dim=1)\n",
    "    sigma_diag = theta_square - theta ** 2\n",
    "    torch.nn.utils.vector_to_parameters(theta, model.parameters())\n",
    "    test_losses = [test_step(batch_x, batch_y, model, criterion) for batch_x, batch_y in test_loader]\n",
    "    # print(f\"Finished SWAG.     Best test loss: {np.mean(test_losses):.5f}\")\n",
    "    return theta, sigma_diag, D, thetas\n",
    "\n",
    "def multitiply_grads(model, lr_multipliers):\n",
    "    start_ind = 0\n",
    "    for params in model.parameters():\n",
    "        shape = params.shape\n",
    "        total_len = params.reshape(-1).shape[0]\n",
    "        multipliers = lr_multipliers[start_ind:(start_ind + total_len)].reshape(shape)\n",
    "        start_ind += total_len\n",
    "        params.grad = params.grad * multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWAG LOOP\n",
    "\n",
    "# for dataset_name in DATASETS[:3]:\n",
    "#     run_SWAG(dataset_name, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "dataset: boston_housing, rows: 506, columns: 13, range of x: [0.0, 711.0], range of y: [5.0, 50.0]\n",
      "Finished Training. Best validation loss: 0.13997 in epoch 258\n",
      "SGD RMSE: 0.346\n",
      "SWAG_lr:  0.01000\n",
      "RMSE: 0.367, PICP: 0.255, MPIW:0.196\n",
      "---MOD---\n",
      "RMSE: 0.349, PICP: 0.059, MPIW:0.028\n",
      "RMSE: 0.350, PICP: 0.098, MPIW:0.069\n",
      "RMSE: 0.353, PICP: 0.078, MPIW:0.084\n",
      "RMSE: 0.363, PICP: 0.235, MPIW:0.138\n",
      "RMSE: 0.379, PICP: 0.275, MPIW:0.244\n",
      "RMSE: 0.396, PICP: 0.412, MPIW:0.373\n",
      "RMSE: 0.415, PICP: 0.451, MPIW:0.428\n",
      "RMSE: 0.429, PICP: 0.451, MPIW:0.443\n",
      "RMSE: 0.425, PICP: 0.510, MPIW:0.526\n",
      "RMSE: 0.449, PICP: 0.529, MPIW:0.597\n",
      "RMSE: 0.456, PICP: 0.549, MPIW:0.609\n",
      "RMSE: 0.437, PICP: 0.627, MPIW:0.704\n",
      "========================================================================================\n",
      "dataset: concrete, rows: 1030, columns: 8, range of x: [0.0, 1145.0], range of y: [2.33, 82.6]\n",
      "Finished Training. Best validation loss: 0.35406 in epoch 241\n",
      "SGD RMSE: 0.567\n",
      "SWAG_lr:  0.20000\n",
      "RMSE: 0.355, PICP: 0.777, MPIW:0.928\n",
      "---MOD---\n",
      "RMSE: 0.560, PICP: 0.019, MPIW:0.037\n",
      "RMSE: 0.553, PICP: 0.058, MPIW:0.069\n",
      "RMSE: 0.541, PICP: 0.078, MPIW:0.128\n",
      "RMSE: 0.516, PICP: 0.155, MPIW:0.234\n",
      "RMSE: 0.487, PICP: 0.262, MPIW:0.360\n",
      "RMSE: 0.453, PICP: 0.388, MPIW:0.531\n",
      "RMSE: 0.440, PICP: 0.447, MPIW:0.575\n",
      "RMSE: 0.437, PICP: 0.515, MPIW:0.663\n",
      "RMSE: 0.420, PICP: 0.505, MPIW:0.624\n",
      "RMSE: 0.410, PICP: 0.553, MPIW:0.705\n",
      "RMSE: 0.385, PICP: 0.602, MPIW:0.715\n",
      "RMSE: 0.376, PICP: 0.680, MPIW:0.750\n",
      "========================================================================================\n",
      "dataset: energy_heating_load, rows: 768, columns: 8, range of x: [0.0, 808.5], range of y: [6.01, 43.1]\n",
      "Finished Training. Best validation loss: 0.07386 in epoch 332\n",
      "SGD RMSE: 0.246\n",
      "SWAG_lr:  0.10000\n",
      "RMSE: 0.211, PICP: 0.818, MPIW:0.421\n",
      "---MOD---\n",
      "RMSE: 0.241, PICP: 0.052, MPIW:0.025\n",
      "RMSE: 0.237, PICP: 0.104, MPIW:0.043\n",
      "RMSE: 0.232, PICP: 0.156, MPIW:0.066\n",
      "RMSE: 0.225, PICP: 0.208, MPIW:0.093\n",
      "RMSE: 0.222, PICP: 0.273, MPIW:0.105\n",
      "RMSE: 0.217, PICP: 0.325, MPIW:0.134\n",
      "RMSE: 0.214, PICP: 0.377, MPIW:0.165\n",
      "RMSE: 0.208, PICP: 0.740, MPIW:0.295\n",
      "RMSE: 0.191, PICP: 0.831, MPIW:0.674\n",
      "RMSE: 0.179, PICP: 0.896, MPIW:0.946\n",
      "RMSE: 0.164, PICP: 0.935, MPIW:1.024\n",
      "RMSE: 0.175, PICP: 0.935, MPIW:0.934\n",
      "========================================================================================\n",
      "dataset: kin8nm, rows: 8192, columns: 8, range of x: [-1.5706812, 1.5707529], range of y: [0.040165378, 1.4585206]\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "S = 500\n",
    "weight_decay = 0\n",
    "multiplier = 2\n",
    "tolerance = 0.05\n",
    "# for dataset_name in ['boston_housing']:\n",
    "# for dataset_name in ['yacht']:\n",
    "# for dataset_name in ['year_prediction_msd']:\n",
    "# for dataset_name in ['naval_compressor_decay']:\n",
    "for dataset_name in DATASETS[:5]:\n",
    "    run_SWAG(dataset_name, weight_decay=0)\n",
    "    x_train, y_train, x_test, y_test, _, _ = load_dataset(dataset_name, verbose=False)\n",
    "    batch_size = x_train.shape[0]//9\n",
    "    model = create_model(x_train, layer_dims=[50], verbose=False)\n",
    "#     train_model(x_train, y_train, x_test, y_test, model, dataset_name, lr=0.001, epochs=50000, verbose=False, \n",
    "#                 batch_size=batch_size, weight_decay=weight_decay)\n",
    "    model = load_model(model, f\"best_model_weights-{dataset_name}.pth\", verbose=False)\n",
    "    y_pred = get_test_predictions(x_train, y_train, x_test, y_test, model)\n",
    "#     print(f\"SGD RMSE: {RMSE(y_pred, y_test):.3f}\")\n",
    "    print(\"---MOD---\")\n",
    "    theta_epoch = torch.nn.utils.parameters_to_vector(model.parameters()).detach().cpu().clone()\n",
    "    lr_multipliers = torch.ones_like(theta_epoch).float()\n",
    "    for rounds in range(12):\n",
    "        try:\n",
    "#             print(0.001 * (2 ** (rounds)))\n",
    "            model = load_model(model, f\"best_model_weights-{dataset_name}.pth\", verbose=False)\n",
    "            theta_swa, sigma_diag, D, thetas = train_SWAG_mod(x_train, y_train, x_test, y_test, model, K, lr_multipliers,\n",
    "                                                          verbose=False, lr=0.001, batch_size=batch_size, \n",
    "                                                          weight_decay=weight_decay, epochs=50)\n",
    "            weight_series = torch.stack(thetas).numpy()\n",
    "            step_plot = weight_series.shape[1] // 5\n",
    "            weight_series -= weight_series.mean(axis=0, keepdims=True)\n",
    "            weight_series /= weight_series.std(axis=0, keepdims=True)\n",
    "#             plt.plot(weight_series[:,::step_plot], alpha=0.3)\n",
    "#             plt.show()\n",
    "            coeffs = calculate_coeffs(weight_series, False)\n",
    "#             print(coeffs)\n",
    "            lr_multipliers[np.abs(coeffs) > tolerance] *= multiplier\n",
    "#             print(lr_multipliers)\n",
    "            sigma_diag = torch.clamp(sigma_diag, min=1e-10)\n",
    "            samples = sample_from_SWAG(x_train, y_train, x_test, y_test, model, theta_swa, sigma_diag, D, K, S)\n",
    "            samples_array = np.concatenate(samples, axis=1)\n",
    "            y_pred = samples_array.mean(axis=1, keepdims=True)\n",
    "            y_l = np.percentile(samples_array, 2.5, axis=1, keepdims=True)\n",
    "            y_u = np.percentile(samples_array, 97.5, axis=1, keepdims=True)\n",
    "            print(f\"RMSE: {RMSE(y_pred, y_test):.3f}, PICP: {np.mean((y_l < y_test) & (y_test < y_u)):.3f}, MPIW:{np.mean(y_u - y_l):.3f}\")\n",
    "        except Exception as e:\n",
    "#             raise e\n",
    "            print(\"Some error\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probai",
   "language": "python",
   "name": "probai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
